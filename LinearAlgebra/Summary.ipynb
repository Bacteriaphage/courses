{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elimination with matrices\n",
    "\n",
    "### Elimination matrix(LU factorization)\n",
    "\n",
    "Suppose we have a invertiable matrix A and we want to use Elimination matrix to solve this linear system:\n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & 2 & 1 \\\\ 3 & 8 & 1 \\\\ 0 & 4 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$E_{21} = \\begin{bmatrix} 1 & 0 & 1 \\\\ -3 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$E_{21}A = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 2 & -2 \\\\ 0 & 4 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$E_{32} = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & -2 & 1 \\end{bmatrix}$$\n",
    "\n",
    "$$E_{32}E_{21}A = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 2 & -2 \\\\ 0 & 0 & 5 \\end{bmatrix}$$\n",
    "\n",
    "In general, we need have three elimination matrix for this:\n",
    "\n",
    "$$E_{32}E_{31}E_{21}A = U$$\n",
    "\n",
    "$$A = LU$$\n",
    "\n",
    "$$L = E_{21}^{-1}E_{31}^{-1}E_{32}^{-1}$$\n",
    "\n",
    "### Permutation matrix\n",
    "\n",
    "Permutate row, change I matrix to P and do PA\n",
    "\n",
    "Permutate column, change I matrix to P and do AP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplication and inverse matrices\n",
    "\n",
    "### block multiplication\n",
    "$$A = \\begin{bmatrix} A_1 & A_2 \\\\ A_3 & A_4 \\end{bmatrix}$$\n",
    "\n",
    "$$B = \\begin{bmatrix} B_1 & B_2 \\\\ B_3 & B_4 \\end{bmatrix}$$\n",
    "\n",
    "$$C = AB$$\n",
    "\n",
    "$$C_1 = A_1B_1+A_2B_3$$\n",
    "\n",
    "### proof of no inverse(singular)\n",
    "\n",
    "If A is singular and x is a non-zero vector which we have following:\n",
    "$$Ax = 0$$\n",
    "\n",
    "Assume A has inverse, then:\n",
    "$$A^{-1}Ax = 0$$\n",
    "\n",
    "$$Ix = 0$$\n",
    "x vector have to be zero, but it's not, so singular matrix A cannot have inverse matrix.\n",
    "\n",
    "### Gauss-Jordan inverse calculation\n",
    "\n",
    "If A have inverse matrix, then:\n",
    "\n",
    "$$\\begin{bmatrix} A & I \\end{bmatrix} -> \\begin{bmatrix} I & A^{-1} \\end{bmatrix}$$\n",
    "\n",
    "the reason is we multiply $A^{-1}$ inverse to both A and I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LU factorization\n",
    "\n",
    "### inverse order\n",
    "$$(AB)^{-1} = B^{-1}A^{-1}$$\n",
    "\n",
    "### factorization\n",
    "\n",
    "$$A = LDU$$\n",
    "\n",
    "$$EA = E_{32}E_{31}E_{21}A = U$$ (no row exchange)\n",
    "\n",
    "E is hard to compute because it's order makes a multiplier in front will effect later which means 32 multiplier will also multiply 21 multiplier.\n",
    "\n",
    "But L is easy to compute because it just fill inverse multipliers into lower triangle.\n",
    "Example:\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & -2 & 1 \\end{bmatrix}\\begin{bmatrix} 1 & 0 & 1 \\\\ -3 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 1 \\\\ -3 & 1 & 0 \\\\ 6 & -2 & 1 \\end{bmatrix}$$\n",
    "\n",
    "here 6 need to compute. but if we inverse these two matrix:\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 0 & 1 \\\\ 3 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 1 & 0 \\\\ 0 & 2 & 1 \\end{bmatrix} = \\begin{bmatrix} 1 & 0 & 1 \\\\ 3 & 1 & 0 \\\\ 0 & 2 & 1 \\end{bmatrix}$$\n",
    "\n",
    "### permutation\n",
    "\n",
    "$$p^{-1} = P^{T}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transpose, space R^n\n",
    "\n",
    "### Characteristic for inversable matrix\n",
    "\n",
    "$$(AA^{-1})^{T} = I = (A^{-1})^TA^T = (A^T)^{-1}A^T$$\n",
    "\n",
    "$$(A^T)^{-1} = (A^{-1})^T$$\n",
    "\n",
    "### vector space and subspace\n",
    "\n",
    "subspace of a vector space must have following properties:\n",
    "\n",
    "vectors are in the same space if scale opertation and addition operation can get a new vector which also exists in this space.\n",
    "R^3 has subspace R^2 which is a plane going through orignal point. R^2 has subspace R^1 which is a line going through original point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Space and Nullspace\n",
    "\n",
    "### Column space\n",
    "combination of pivot column vectors can build a Column Space:\n",
    "$$A_{4x3}x_{3x1}=b$$\n",
    "vector b is in A column space if and only if it's combination of A column vector\n",
    "\n",
    "### Null space\n",
    "Null space is a combination of x when b = 0, and null space is a subspace.\n",
    "\n",
    "### rank and solve Ax=0\n",
    "after elimination of a column space, we can get pivot column and free column. (echelon)\n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & 2 & 2 & 2 \\\\ 0 & 0 & 2 & 4 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "free column 2, 4\n",
    "\n",
    "pivot column 1, 3\n",
    "\n",
    "this means to find a vector in null space, we can randomly choose x2 and x4 and calculate x1 and x3\n",
    "\n",
    "rank = 2 it means we have 2 povit varibles\n",
    "\n",
    "if A has n columns, then we have n-rank free varibles\n",
    "\n",
    "And null space include all vectors with different free varibles.\n",
    "\n",
    "R = reduced row echelon form\n",
    "\n",
    "$$R = \\begin{bmatrix} 1 & 2 & 0 & -2 \\\\ 0 & 0 & 1 & 2 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "this is easier to get null space vector, the pivot column only depends on free column not other pivot columns.\n",
    "\n",
    "In general:\n",
    "$$R = \\begin{bmatrix} I_{rxr} & F_{rx(n-r)} \\\\ 0 & 0 \\end{bmatrix}$$\n",
    "\n",
    "$$RN = 0$$\n",
    "\n",
    "$$N = \\begin{bmatrix} -F \\\\ I \\end{bmatrix}$$\n",
    "\n",
    "A full column rank which rank = n has null space as zero vector."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve Ax=b\n",
    "\n",
    "the complete solution for x is any particular x + any combination of nullspace vectors\n",
    "$$Ap + An = b + 0 = b$$\n",
    "$$A(p+n) = b$$\n",
    "\n",
    "We have mxn matrix.\n",
    "\n",
    "full column rank will have 1 or 0 solution depending on b. (r = n < m)\n",
    "\n",
    "full row rank will always have infinite solutions for any b. (r = m < n)\n",
    "\n",
    "full column and row rank matrix has 1 and only 1 solution for any b (invertable matrix).\n",
    "\n",
    "non-full column and row rank matrix has infinite solutions or 0 solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence, basis and dimension\n",
    "\n",
    "vector v1, v2, v3... are independent if no combination for c1, c2, c3 (except all zero) will make:\n",
    "$$v_1c_1 + v_2c_2 + v_3c_3 + ... + v_nc_n = 0$$\n",
    "\n",
    "In another way when v1, v2, v3... are columns of A, they are independent of nullspace of A is zero vector. (rank = n)\n",
    "\n",
    "Basis for a space is a sequence of vectors with 2 properties:\n",
    "\n",
    "1. they are independet.\n",
    "2. they span the space.\n",
    "\n",
    "every basis for the space has the same number of vectors and the number is demension.\n",
    "\n",
    "the demention of null space is number of free varibales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Subspaces\n",
    "A is a mxn matrix\n",
    "\n",
    "Column space $C(A) \\ dim=rank$\n",
    "\n",
    "Null space $N(A) \\ dim=n-rank$\n",
    "\n",
    "Row space $C(A^T) \\ dim=rank$\n",
    "\n",
    "Null space of transpose (left null space of A) $N(A^T) \\ dim=m-rank$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matrix space\n",
    "\n",
    "like vector space, we only care about addition and scale, no multiplication.\n",
    "\n",
    "Rank 1 matrix:\n",
    "$$A = uv^T$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph (Electric)\n",
    "\n",
    "Incident matrix for a direct graph, column stands for nodes, row stands for edges.\n",
    "\n",
    "1 is out from a node, -1 is into a node, if there is a loop, then sum of edge rows will be 0.\n",
    "\n",
    "The transpose of this matrix help solve Kirchoff's Current Law, the null space contains current flow which make each node in circuit has balance input and output.\n",
    "$$A^Ty = 0$$\n",
    "The null space of matrix itself represent potential of each node and the rank of null space shows how many ground we need.\n",
    "\n",
    "e is potential differences\n",
    "$$e = Ax$$\n",
    "The basis of the matrix will form a tree.\n",
    "\n",
    "No. of nodes - No. of edges + No. of loops = 1:\n",
    "\n",
    "$$n - m + dim(N(A^T)) = 1$$\n",
    "$$n - m + m - r = 1$$\n",
    "\n",
    "this means circuit should only have one ground point.\n",
    "\n",
    "Because current = 1/resistance * e, if we have a current source f, we get:\n",
    "$$y = Ce$$\n",
    "$$A^TCAx=f$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## orthogonal\n",
    "\n",
    "orthogonal subspace is, any v in subspace A and any u in subspace B is orthognal, e.g. (x, y) and (y, z) are not orthogonal.\n",
    "\n",
    "If 2 subspaces only have intersection as zero vector, then they are orthogonal.\n",
    "\n",
    "For $A_(mxn)$\n",
    "\n",
    "Row space and null space are orthogonal complements in $R^n$\n",
    "\n",
    "$$Ax = 0$$\n",
    "\n",
    "$$\\Sigma C_n(row_n)^Tx = 0$$\n",
    "\n",
    "Nullspace contains all vectors perpendicular with row space.\n",
    "\n",
    "### \"solve\" unsolvable system\n",
    "\n",
    "if we have a linear system:\n",
    "\n",
    "$$Ax = b$$\n",
    "\n",
    "where m > n and we connot solve x since b is not combination of A column vectors.\n",
    "\n",
    "instead of throwing away random noise row to make invertable matrix, we build:\n",
    "\n",
    "$$A^TA\\hat{x} = A^Tb$$\n",
    "\n",
    "$$N(A) = N(A^TA)$$\n",
    "\n",
    "$$rank(A) = rank(A^TA)$$\n",
    "\n",
    "$A^TA$ is invertable only if column vectors in A are independent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Projection\n",
    "\n",
    "a and b are two vectors\n",
    "\n",
    "$$a^T(b-xa) = 0$$\n",
    "\n",
    "$$a^Te = 0$$\n",
    "\n",
    "$$x = \\frac{a^Tb}{a^Ta}$$\n",
    "\n",
    "$$\\hat{x} = ax = \\frac{aa^T}{a^Ta}b = pb$$\n",
    "\n",
    "$a^Ta$ has rank 1 and $p^2 = p$ and $p = p^T$\n",
    "\n",
    "If $Ax=b$ has no solution, then we can solve $A\\hat{x}=p$ instead.\n",
    "\n",
    "Expand a to A, $A^T(b-A\\hat{x}) = 0$, e in $N(A^T)$ and from orthogonal property, we know e is perpendicular with A column space. \n",
    "\n",
    "$$\\hat{x} = (A^TA)^{-1}A^Tb$$\n",
    "$$P = pb = A\\hat{x} = A(A^TA)^{-1}A^Tb$$\n",
    "$$e = (I-p)b$$\n",
    "the projection to column space and A transpose null space build b:\n",
    "$$(P+e)b = b$$\n",
    "\n",
    "Be careful, A is not a square matrix otherwise it is whole space of $R^n$ which b is in A, Here A has m>n and b is not in its column space. So A is not invertable.\n",
    "\n",
    "### Least squares\n",
    "\n",
    "Be careful, least sqaures is not for the point prependicularly project to line distance, the projection and original point have the same x.  \n",
    "\n",
    "suppose we have three points on a plane:(1, 1), (1, 2) and (1, 3), find a line which has the least distance with these points.\n",
    "\n",
    "$$b = cx+d$$\n",
    "\n",
    "$$c + d = 1$$\n",
    "$$c + 2d = 2$$\n",
    "$$c + 3d = 2$$\n",
    "\n",
    "$$A = \\begin{bmatrix} 1 & 1 \\\\ 1 & 2 \\\\ 1 & 3 \\end{bmatrix}\\begin{bmatrix} c \\\\ d \\end{bmatrix} = \\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix}$$\n",
    "\n",
    "Minimize: $\\begin{Vmatrix}Ax-b\\end{Vmatrix}^2 = \\begin{Vmatrix}E\\end{Vmatrix}^2$\n",
    "\n",
    "Minimize: $f(\\hat{c}, \\hat{d}) = (\\hat{c}+\\hat{d}-1)^2+(\\hat{c}+2\\hat{d}-2)^2+(\\hat{c}+3\\hat{d}-2)^2$\n",
    "\n",
    "Do partial derivative on c and d, we get an c, d combo.\n",
    "\n",
    "Also, use $A^TA\\hat{x} = A^Tb$\n",
    "\n",
    "$$\\begin{bmatrix} 3 & 6 \\\\ 6 & 14 \\end{bmatrix}\\begin{bmatrix} \\hat{c} \\\\ \\hat{d} \\end{bmatrix} = \\begin{bmatrix} 5 \\\\ 11 \\end{bmatrix}$$\n",
    "\n",
    "We can get exactly the same c, d combo coming from derivative.\n",
    "\n",
    "$$b = P + e$$\n",
    "\n",
    "$$\\begin{bmatrix} 1 \\\\ 2 \\\\ 2 \\end{bmatrix} = \\begin{bmatrix} \\frac{7}{6} \\\\ \\frac{10}{6} \\\\ \\frac{13}{6} \\end{bmatrix} + \\begin{bmatrix} -\\frac{1}{6} \\\\ \\frac{2}{6} \\\\ -\\frac{1}{6} \\end{bmatrix}$$\n",
    "\n",
    "If A has independent columns then $A^TA$ is invertable.\n",
    "\n",
    "Proof:\n",
    "\n",
    "If $A^TA$ is invertable:\n",
    "$$A^TAx = 0$$\n",
    "$$x^TA^TAx = 0$$\n",
    "$$(Ax)^TAx = 0$$\n",
    "$Ax = 0$\n",
    "Because A has independent columns, so x must not be zero, which also make $A^TA$ is invertable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Orthogonal matrix and Gram-Schmidt\n",
    "\n",
    "### Ortho-normal vectors\n",
    "\n",
    "if $Q$ has Ortho-normal vectors $q_i$, then\n",
    "\n",
    "$$Q^TQ = \\begin{bmatrix} q_1^T \\\\ ... \\\\ q_n^T \\end{bmatrix}\\begin{bmatrix} q_1 & ... & q_n\\end{bmatrix} = I$$\n",
    "\n",
    "if $Q$ is square matrix, the $Q^T=Q^{-1}$\n",
    "\n",
    "projection matrix of orthonogal matrix:\n",
    "\n",
    "$$p = Q(Q^TQ)^{-1}Q^T = QQ^T$$\n",
    "\n",
    "if Q is square: $p = QQ^T$\n",
    "\n",
    "if Q is not square, use property of projection matrix $P = P^2$, so:\n",
    "\n",
    "$$pp = p$$\n",
    "$$QQ^TQQ^T = QQ^T$$\n",
    "\n",
    "Go back to least square problem:\n",
    "\n",
    "$$Q^TQ\\hat{x} = Q^Tb$$\n",
    "$$\\hat{x} = Q^Tb$$\n",
    "\n",
    "### Gram-schmidt\n",
    "\n",
    "We have three independent vector a, b, c, our goal is to generate orthonormal vectors A, B, C.\n",
    "\n",
    "$$A = \\frac{a}{\\begin{Vmatrix}A\\end{Vmatrix}}$$\n",
    "\n",
    "$$B = b - \\frac{A^Tb}{A^TA}$$\n",
    "\n",
    "$$C = c - \\frac{A^Tc}{A^TA} - \\frac{B^Tc}{B^TB}$$\n",
    "\n",
    "$$Q = \\begin{bmatrix} A & B & C\\end{bmatrix}$$\n",
    "\n",
    "Basically it's $e = b - P$\n",
    "\n",
    "$$M = \\begin{bmatrix} a & b & c\\end{bmatrix}$$\n",
    "\n",
    "$$M = QR$$\n",
    "\n",
    "R is upper triangular matrix. This is because in Gram-schmidt process, only later orthonormal vector will perpendicular to previous original vector, say, $a^TB = a^TC = b^TC = 0$, $b^TA \\ne 0$ , for example:\n",
    "\n",
    "$$M = \\begin{bmatrix} 1 & 1 \\\\ 1 & 0 \\\\ 1 & 2 \\end{bmatrix}$$\n",
    "\n",
    "$$Q = \\begin{bmatrix} \\frac{1}{\\sqrt3} & 0 \\\\ \\frac{1}{\\sqrt3} & -\\frac{1}{\\sqrt2} \\\\ \\frac{1}{\\sqrt3} & \\frac{1}{\\sqrt2} \\end{bmatrix}$$\n",
    "\n",
    "$$\\begin{bmatrix} 1 & 0 & 2 \\end{bmatrix}\\begin{bmatrix} \\frac{1}{\\sqrt3} \\\\ \\frac{1}{\\sqrt3} \\\\ \\frac{1}{\\sqrt3} \\end{bmatrix} \\ne 0$$\n",
    "\n",
    "And R will be:\n",
    "\n",
    "$$R = \\begin{bmatrix} a^TA & b^TA & c^TA \\\\ a^TB & b^TB & c^TB \\\\ a^TC & b^TC & c^TC \\end{bmatrix}$$\n",
    "\n",
    "It's not apply for any arbitrary $N$ which we want to get $M = NR$, only for $Q$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinants\n",
    "\n",
    "Property of Determinant:\n",
    "1. $det I = 1$\n",
    "2. exchange row reverse sign of det\n",
    "3. $\\begin{vmatrix} ta & tb \\\\ c & d \\end{vmatrix} = t\\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix}$\n",
    "4. $\\begin{vmatrix} a+a' & b+b' \\\\ c & d \\end{vmatrix} = \\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix} + \\begin{vmatrix} a' & b' \\\\ c & d \\end{vmatrix}$\n",
    "5. if two rows are equal, then $det = 0$\n",
    "6. Substract l * row i from row k will not change determinent.\n",
    "$$\\begin{vmatrix} a & b \\\\ c-la & d-lb \\end{vmatrix} = \\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix} + \\begin{vmatrix} a & b \\\\ la & lb \\end{vmatrix} == \\begin{vmatrix} a & b \\\\ c & d \\end{vmatrix} + l0$$\n",
    "7. if a row is 0, then $det A = 0$\n",
    "8. upper trianguler matrix has determinent as product of all pivots.\n",
    "$$A = \\begin{vmatrix} d1 & * \\\\ 0 & d2 \\end{vmatrix} = \\begin{vmatrix} d1 & 0 \\\\ 0 & d2 \\end{vmatrix} = d1d2\\begin{vmatrix} 1 & 0 \\\\ 0 & 1 \\end{vmatrix} == d1d2$$\n",
    "9. $det AB = (det A)(det B)$\n",
    "10. $det A^T = det A$\n",
    "$$A = LU$$\n",
    "$$\\begin{vmatrix} U^TL^T \\end{vmatrix} = \\begin{vmatrix} U^T \\end{vmatrix} \\begin{vmatrix} L^T \\end{vmatrix} = \\begin{vmatrix} U \\end{vmatrix} \\begin{vmatrix} L \\end{vmatrix} = \\begin{vmatrix} LU \\end{vmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formula for determinent, cofactor formular, triagonal matrices\n",
    "\n",
    "### Formula for determinent\n",
    "use det property 4 and 2, we can split orignal det to n! permutation det. and then swap rows in each det to get diagnal det, and get final result\n",
    "\n",
    "$$det A = \\Sigma\\pm a_{1\\alpha}a_{2\\beta}a_{3\\gamma}...$$\n",
    "$$ (\\alpha,\\beta,\\gamma...) = perm(1, 2, 3...)$$\n",
    "\n",
    "### cofactor\n",
    "\n",
    "cofactor of $a_{ij}$ is $\\pm$det of n-1 matrix without row i and column j. if i+j is even then use plus, otherwise use minus.\n",
    "\n",
    "$$Cofactor \\ of \\ a_{ij} = C_{ij}$$\n",
    "$$det A = a_{11}C_{11}+a_{12}C_{12}+...+a_{1n}C_{1n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cramer's rule, inverse matrix and volume\n",
    "\n",
    "$$A^{-1} = \\frac{1}{det A}C^T$$\n",
    "\n",
    "To prove this, multiply A on both side:\n",
    "\n",
    "$$\\frac{1}{det A}AC^T = I$$\n",
    "\n",
    "in diagonal position, we get exactly $det A$, but in other place, the calculation matrix will be singular which generate 0:\n",
    "\n",
    "for example for matrix $\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}$ the $C^T$ is $\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}$, the calculation matrix for $(0,1)$ is $\\begin{bmatrix} a & b \\\\ a & b \\end{bmatrix}$ which is singular. Expand it to nxn matrix, the calculation matrix for non-diagonal matrix will have two same row.\n",
    "\n",
    "$$Ax = b$$\n",
    "$$x = A^{-1}b = \\frac{1}{det A}C^Tb$$\n",
    "\n",
    "### Cramer's rule\n",
    "\n",
    "$$x_i = \\frac{det B_i}{det A}$$\n",
    "\n",
    "where $B_i$ is a matrix which replace column i of A with b. But this is not very practical.\n",
    "\n",
    "### Volume\n",
    "\n",
    "The volume of box formed by vectors is the absolute value of $det A$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors\n",
    "\n",
    "$$Ax = \\lambda x$$\n",
    "\n",
    "Fact: \n",
    "1. sum of $\\lambda$ equals to sum of dianogal element called trace.\n",
    "2. det of matrix equals to product from eigenvalues.\n",
    "\n",
    "$$(A+nI)x = (\\lambda+n)x$$\n",
    "\n",
    "if $Bx = \\alpha x$, $(A+B)x \\ne (\\lambda+\\alpha)x$, because x for A and B might not be the same.\n",
    "\n",
    "Symmetric matrix will have eigenvalues in R, bad matrix like $\\begin{bmatrix}0 & 1 \\\\ -1 & 0\\end{bmatrix}$ which has i and -i as eigenvalues.\n",
    "\n",
    "Triangluar matrix will have eigenvalues as diagonal element.\n",
    "\n",
    "Assume S contains all eigenvectors $x_i$, then:\n",
    "\n",
    "$$AS = A\\begin{bmatrix}x_1 & x_2 & .. & x_n\\end{bmatrix} == \\begin{bmatrix}\\lambda_1x_1 & \\lambda_2x_2 & .. & \\lambda_n x_n\\end{bmatrix} = \\begin{bmatrix}x_1 & x_2 & .. & x_n\\end{bmatrix}\\begin{bmatrix}\\lambda_1 & 0 & ... \\\\ 0 & \\lambda_2 & ... \\\\ ...\\\\ 0 & ... & \\lambda_n \\end{bmatrix} = S\\Lambda$$\n",
    "\n",
    "$$S^{-1}AS = \\Lambda$$\n",
    "\n",
    "$$A = S\\Lambda S^{-1}$$\n",
    "\n",
    "Another importent factorization.\n",
    "\n",
    "$$A^2 = S\\Lambda S^{-1}S\\Lambda S^{-1} = S\\Lambda^2 S^{-1}$$\n",
    "\n",
    "$$A^n = S\\Lambda^n S^{-1}$$\n",
    "\n",
    "Example:\n",
    "\n",
    "$$u_{k+1} = Au_k$$\n",
    "\n",
    "$$u_k = A^ku_0$$\n",
    "\n",
    "Assume $u_0$ is combination of eigenvectors of A, then:\n",
    "\n",
    "$$u_k = A^kSc = A^{k-1}ASc = A^{k-1}S\\Lambda c = S \\Lambda^kc$$\n",
    "\n",
    "So:\n",
    "\n",
    "$$A^k = S\\Lambda^kS^{-1}$$\n",
    "\n",
    "Theorem: $A^k->0$ as $k->\\infty$ if all $\\vert{\\lambda_i}\\vert < 1$, it mean in complex zone, complex number has radius less than 1.\n",
    "\n",
    "A will have n indenpendent eigenvalue if all the $\\lambda$ are different.\n",
    "\n",
    "A having repeated eigenvalues may or may not have independent eigenvectors. E.g. $A = I$ and $A = \\begin{bmatrix}2 & 1 \\\\ 0 & 2\\end{bmatrix}$\n",
    "\n",
    "Example: check grow speed of a Fibonacci array:\n",
    "\n",
    "$$F_{k+2} = F_{k+1} + F_{k}$$\n",
    "\n",
    "$$F_{k+1} = F_{k+1}$$\n",
    "\n",
    "$$u_k = \\begin{bmatrix}F_{k+1} \\\\ F_k\\end{bmatrix}$$\n",
    "\n",
    "$$u_{k+1} = Au_k$$\n",
    "\n",
    "$$\\begin{bmatrix}F_{k+1} + F_{k} \\\\ F_{k+1}\\end{bmatrix} = A\\begin{bmatrix}F_{k+1} \\\\ F_k\\end{bmatrix}$$\n",
    "\n",
    "$$A = \\begin{bmatrix}1 & 1 \\\\ 1 & 0\\end{bmatrix}$$\n",
    "Do not to elimnation here!!!\n",
    "$$\\vert A-\\lambda I\\vert = 0$$\n",
    "\n",
    "$$\\lambda^2 - \\lambda -1 = 0$$\n",
    "\n",
    "$$\\lambda = \\frac{1\\pm\\sqrt{5}}{2}$$\n",
    "\n",
    "$$\\begin{bmatrix}F_{k+1} \\\\ F_{k}\\end{bmatrix} = S\\Lambda^kS^{-1}\\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "We can get eigenvector from $(A-\\lambda I)x = 0$: $\\begin{bmatrix}\\lambda_1 \\\\ 1\\end{bmatrix}$ and $\\begin{bmatrix}\\lambda_2 \\\\ 1\\end{bmatrix}$\n",
    "\n",
    "Then, we get result. But the growing speed should be $\\frac{1+\\sqrt{5}}{2}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential equation\n",
    "\n",
    "Suppose we have two differential equations:\n",
    "\n",
    "$$\\frac{du_1}{dt} = -u_1 + 2u_2$$\n",
    "\n",
    "$$\\frac{du_2}{dt} = u_1 - 2u_2$$\n",
    "\n",
    "$$u_0 = \\begin{bmatrix}1 \\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Then we have the first order differential equation matrix as $A = \\begin{bmatrix}-1 & 2 \\\\ 1 & -2\\end{bmatrix}$\n",
    "\n",
    "We can use our observation first get $\\lambda_1 = 0$, then use fact that sum of diagonal equals to sum of eigenvalues, we get $\\lambda_2 = -3$\n",
    "\n",
    "We also get eigenvector as $x_1 = \\begin{bmatrix}2 \\\\ 1\\end{bmatrix}$ and $x_1 = \\begin{bmatrix}1 \\\\ -1\\end{bmatrix}$\n",
    "\n",
    "Then the solution of this differential equation system is:\n",
    "\n",
    "$$u(t) = c_1e^{\\lambda_1t}x_1 + c_2e^{\\lambda_2t}x_2$$\n",
    "\n",
    "Plugin initial value $u_0$\n",
    "\n",
    "we get $c_1 = c_2 = \\frac{1}{3}$\n",
    "\n",
    "We can get such format is because of the property of exponential.\n",
    "\n",
    "Some observation here:\n",
    "\n",
    "1. To make the system stable, $u(t) -> 0$, every $\\lambda$ should be less than 0. If $\\lambda$ has imaginate part, we can omit it, since $\\vert e^{nit}\\vert = 1$, it is just $(\\sin nt, \\cos nt)$ \n",
    "2. To make the system has steady state, $u(t)->number$, some $\\lambda$ should be 0 and other should less than 0.\n",
    "3. To make the system explode, make any Re $\\lambda > 0$\n",
    "\n",
    "For 2x2 matrix, we can check $det$ and sum of diagonal elements to find whether the system is stable.\n",
    "\n",
    "When t = 0, we can easily see:\n",
    "$$\\begin{bmatrix}2 & 1 \\\\ 1 & -1\\end{bmatrix}\\begin{bmatrix}c_1 \\\\ c_2\\end{bmatrix} = u_0$$\n",
    "\n",
    "so:\n",
    "\n",
    "$$u_0 = Sc$$\n",
    "\n",
    "### decouple u\n",
    "\n",
    "We can use $S$ to decouple $u$:\n",
    "$$u = Sv$$\n",
    "plugin this to $\\frac{du}{dt} = Au$, we get:\n",
    "\n",
    "$$S\\frac{dv}{dt} = ASv$$\n",
    "$$\\frac{dv}{dt} = S^{-1}ASv = \\Lambda v$$\n",
    "$$\\frac{dv_1}{dt} = \\lambda_1v_1 \\ ... \\ \\frac{dv_2}{dt} = \\lambda_2v_2 \\$$\n",
    "\n",
    "This is why it called decouple, because $\\frac{dv_i}{dt}$ only depends on $v_i$\n",
    "\n",
    "$$v(t) = e^{\\Lambda t}v(0)$$\n",
    "\n",
    "suppose we can get the same result for u as v:\n",
    "\n",
    "$$u(t) = e^{At}u(0)$$\n",
    "\n",
    "apply taylor expansion to $e^x = 1 + x + \\frac{x^2}{2} +...+ \\frac{x^n}{n!}$\n",
    "\n",
    "$$e^{At} = I + At + \\frac{(At)^2}{2} +...+ \\frac{(At)^n}{n!}$$\n",
    "\n",
    "$$e^{At} = SS^{-1} + S\\Lambda S^{-1}t + \\frac{S\\Lambda^2 S^{-1}t^2}{2} +...+ \\frac{S\\Lambda^n S^{-1}t^n}{n!}$$\n",
    "\n",
    "$$e^{At} = Se^{\\Lambda t}S^{-1}$$\n",
    "\n",
    "$$e^{\\Lambda t} = \\begin{bmatrix}e^{\\lambda_1t} & 0 & ... \\\\ ... \\\\ 0 & ...& e^{\\lambda_nt}\\end{bmatrix}$$\n",
    "\n",
    "For high order differential equation, we can normally get $A$ as:\n",
    "\n",
    "$$\\begin{bmatrix}co_1 & co_2 & ... & co_n \\\\ 1 & ... \\\\ 0 & 1 & ... \\\\ ... \\\\ 0 & ... & 1 & 0\\end{bmatrix}$$\n",
    "\n",
    "For example: $y'' + by' + ky = 0$\n",
    "$$\\begin{bmatrix}y'' \\\\ y'\\end{bmatrix} = \\begin{bmatrix}-b & -k\\\\ 1 & 0 \\end{bmatrix}\\begin{bmatrix}y' \\\\ y\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov matrices and Fourier series\n",
    "\n",
    "### Markov\n",
    "Property:\n",
    "1. All entries $\\ge 0$\n",
    "2. All columns add up to 1\n",
    "\n",
    "Fact:\n",
    "1. 1 is an eigenvalue\n",
    "$$det(A-I) = 0$$\n",
    "according to preperty 2, if we minus 1 for each row, sum up columns will be 0, then we have a combination to make it 0.\n",
    "\n",
    "2. all other $\\vert\\lambda\\vert < 1$\n",
    "\n",
    "3. steady state:\n",
    "$$u_k = A^ku_0 = c_1\\lambda_1x_1+c_2\\lambda_2x_2+...+c_n\\lambda_nx_n$$\n",
    "$$k->\\infty \\ then \\ u_k = const$$\n",
    "\n",
    "Some useful prove and reminder:\n",
    "\n",
    "1. Eigenvalue of $A$ is the same as Eigenvalue of $A^T$\n",
    "$$det(A-\\lambda I)=0=det((A-\\lambda I)^T)=det(A^T-\\lambda I)$$\n",
    "2. Eigenvector for $A$ and $A^T$ are different for the same $\\lambda$\n",
    "3. For Markov matrices, eigenvalue one will have eigenvector $\\begin{bmatrix}1\\\\1\\\\..\\\\1\\end{bmatrix}$ for $A^T$, and $x$ for eigenvector of A. And we can get a steady state from $x$.\n",
    "\n",
    "### Fourier series\n",
    "\n",
    "Let start from projection. is we have a vector $v$ and we need to project it to orthogonal $q_1, q_2, ...,q_n$, then we have:\n",
    "\n",
    "$$v = \\sum_{i=1}^{n}c_iq_i$$\n",
    "\n",
    "To calculate each coefficient, we need to do:\n",
    "\n",
    "$$q_i^Tv = 0 + 0 +...+c_i+0+..+0$$\n",
    "\n",
    "$$c_i = q_i^Tv$$\n",
    "\n",
    "Use matrix to show this is:\n",
    "\n",
    "$$Qc = v$$\n",
    "\n",
    "$$c = Q^{-1}v = Q^Tv$$\n",
    "\n",
    "If we have some orthogonal basis, we can take advantage of this property.\n",
    "\n",
    "Let now move to function world. We have $1$, $\\sin x$, $\\cos x$, $\\sin 2x$, $\\cos 2x$ ..... orthogonal with each other, then we can project any function to these basis.\n",
    "\n",
    "Instead of inner product for discrete vectors, we need integral for functions, that is:\n",
    "\n",
    "$$f^Tg = \\int f(x)g(x)dx$$\n",
    "\n",
    "we have following formula:\n",
    "\n",
    "$$f(x) = c_0 + c_1\\sin x + c_2\\cos x + c_3\\sin 2x + c_4\\cos 2x + ...$$\n",
    "\n",
    "to calculate c_1:\n",
    "\n",
    "$$\\int_0^{2\\pi}f(x)\\sin xdx = c_1\\int_0^{2\\pi}\\sin^2 xdx = c_1\\int_0^{2\\pi} \\frac{1-cos 2x}{2}dx = \\pi c_1$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Symmetric matrix\n",
    "\n",
    "Fact for a real symmetric matrix:\n",
    "\n",
    "1. The eigenvalues are real number.\n",
    "2. The eigenvectors are orthogonal.\n",
    "\n",
    "$$A = Q\\Lambda Q^{-1} = Q\\Lambda Q^T = (Q\\Lambda Q^T)^T = A^T$$\n",
    "\n",
    "Proof of real eigenvalue:\n",
    "\n",
    "$$Ax = \\lambda x$$\n",
    "$$\\bar{x}^TAx = \\lambda \\bar{x}^Tx$$\n",
    "\n",
    "$$\\bar{A}\\bar{x} = \\bar{\\lambda}\\bar{x}$$\n",
    "$$\\bar{x}^T\\bar{A}^T = \\bar{x}^T\\bar{\\lambda}$$\n",
    "$$\\bar{x}^T\\bar{A}^Tx = \\bar{x}^Tx\\bar{\\lambda}$$\n",
    "For real symmetric A, $A = \\bar{A}^T$:\n",
    "$$\\lambda = \\bar{\\lambda}$$\n",
    "\n",
    "For any complex A, if $A = \\bar{A}^T$, then the two property are also true.\n",
    "\n",
    "Another fact: the signs of pivot are the same as the signs of eigenvalues.\n",
    "\n",
    "### Positive definite matrix\n",
    "\n",
    "Symmetric matrices with all eigenvalues are positive.\n",
    "\n",
    "Four way to test:\n",
    "1. All eigenvalues are positive\n",
    "2. Determinent of all left top cornor matrices are positive\n",
    "3. All pivots should be positive\n",
    "4. For any non-zero x, $x^TAx > 0$\n",
    "\n",
    "Positive semidefinite matrix has $\\lambda = 0$\n",
    "\n",
    "Proof of 4:\n",
    "\n",
    "For a matrix $\\begin{bmatrix}a & b \\\\ b & c\\end{bmatrix}$\n",
    "\n",
    "$$\\begin{bmatrix}x & y\\end{bmatrix}\\begin{bmatrix}a & b \\\\ b & c\\end{bmatrix}\\begin{bmatrix}x \\\\ y\\end{bmatrix} = ax^2+2bxy+cy^2$$\n",
    "\n",
    "use fact 2, we have $ac-b^2 > 0$\n",
    "\n",
    "then we tranform above to a complete square formula:\n",
    "\n",
    "$$x^TAx = a(x+\\frac{b}{a}y)^2 + (c-\\frac{b^2}{a})y^2$$\n",
    "\n",
    "we can see, if $A$ is a positive definite matrix, then $x^TAx > 0$\n",
    "\n",
    "More observation, in function graph, if A is a PDM, then the graph should be a bowl open to top and origin is the minimum. If A is not PDM, the minimum is not 0 and the value will be less than 0.\n",
    "\n",
    "From calculus perspective, to caluclate minimum, we need $\\frac{df}{dx} = 0$ and $\\frac{d^2f}{dx^2} > 0$ which mean the slope keeps increasing. expand it to n dimensions:\n",
    "\n",
    "$$\\begin{bmatrix}f_{xx} & f_{xy} & f_{xz} \\\\ f_{yx} & f_{yy} & f_{yz} \\\\ f_{zx} & f_{zy} & f_{zz} \\end{bmatrix}$$\n",
    "\n",
    "Only this matrix is PDM, then the funtion will have minimum point. (property of partial derivative: $f_{ij}=f_{ji}$)\n",
    "\n",
    "Another observation, for $f(x, y) = ax^2+2bxy+cy^2$, if we make $f(x, y) = c$, then we get an elipse which has eigenvalues as the length of principle and eigenvectors are the principle directions.\n",
    "\n",
    "For any matrix A $A^TA$ is a PDM if A is a full rank matrix:\n",
    "\n",
    "$$x^TA^TAx = (Ax)^T(Ax) = length \\ of\\ vector > 0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar Matrix\n",
    "\n",
    "$A$ and $B$ are similar when for some M:\n",
    "$$B = M^{-1}AM$$\n",
    "\n",
    "The similar matrices will have the same eigenvalues:\n",
    "\n",
    "$$Ax = \\lambda x$$\n",
    "$$M^{-1}AMM^{-1}x = \\lambda M^{-1}x = BM^{-1}x = \\lambda M^{-1}x$$\n",
    "$$Bx = \\lambda x$$\n",
    "\n",
    "If A has different eigenvalues, then the big family of matrices has multiple families.\n",
    "\n",
    "For example: $\\lambda_1 = \\lambda_2 = 4$, we have two families:\n",
    "$$\\begin{bmatrix}4 & 0 \\\\ 0 & 4\\end{bmatrix}$$\n",
    "$$and$$\n",
    "$$\\begin{bmatrix}4 & 1 \\\\ 0 & 4\\end{bmatrix}$$\n",
    "\n",
    "For first matrix, after apply $M^{-1}AM$, we always get itself, so 1 and 2 are not similar.\n",
    "\n",
    "The second matrix is not diagonalizable, because it only has one eigenvector. The second matrix is called $Jordan\\ form$, which gives the most closed form to a diagonalizable matrix.\n",
    "\n",
    "Jordan block: $J_i = \\begin{bmatrix}\\lambda_i & 1 \\\\ 0 & \\lambda_i & 1 \\\\ ... \\\\ 0 & 0 & ... & \\lambda_i\\end{bmatrix}$\n",
    "\n",
    "$Jordan\\ theorom$ Every A is similar to a Jordan matrix J:\n",
    "$$J = \\begin{bmatrix}J_1 \\\\ 0 & J_2 \\\\ ... \\\\ 0 & 0 & ... & J_d\\end{bmatrix}$$\n",
    "\n",
    "the number of blocks shows the number of eigenvectors.\n",
    "\n",
    "For example:\n",
    "\n",
    "$\\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 0\\end{bmatrix}$\n",
    "and\n",
    "$\\begin{bmatrix} 0 & 1 & 0 & 0 \\\\ 0 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0\\end{bmatrix}$\n",
    "\n",
    "have the same eigenvalues 0,0,0,0, and also they have the same rank, same number of eigenvector, but they are not similar, since they don't have the same Jordan Form.\n",
    "\n",
    "The first one is $\\begin{bmatrix}J_{3x3} \\\\ 0 & J_{1x1}\\end{bmatrix}$, the second one is $\\begin{bmatrix}J_{2x2} \\\\ 0 & J_{2x2}\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular value decomposition\n",
    "\n",
    "For every square matrix $A$, we want to decompose it to $U\\Sigma V^T$, where $U$ and $V$ are ortho-normal matrices and $\\Sigma$ is a diagonal matrix.\n",
    "\n",
    "for any base vector $u$ in basis $U$, our $A$ can map it to another vector $v$ in basis $V$ with some parameters $\\sigma$:\n",
    "\n",
    "$$\\sigma_i u_i = Av_i$$\n",
    "\n",
    "$$A = U\\begin{bmatrix}\\sigma_1 \\\\ 0 & \\sigma_2 \\\\ ... \\\\ 0 & 0 &...&\\sigma_n\\end{bmatrix}V^T$$\n",
    "\n",
    "To caluclate $U$ and $V$, we need to use PDM matrix $AA^T$ and $A^TA$\n",
    "\n",
    "$$A^TA = V\\Sigma^2V^T$$\n",
    "\n",
    "$$AA^T = U\\Sigma^2U^T$$\n",
    "\n",
    "We can calulate them by calculating eigenvalues and eigenvectors of these two matrices.\n",
    "\n",
    "Example 1:\n",
    "\n",
    "$$A = \\begin{bmatrix}4 & 4 \\\\ -3 & 3\\end{bmatrix}$$\n",
    "\n",
    "$$A^TA = \\begin{bmatrix}25 & 7 \\\\ 7 & 25\\end{bmatrix}$$\n",
    "$$V = \\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & -1 \\\\ 1 & 1\\end{bmatrix}\\ \\Lambda =\\begin{bmatrix}32 & 0 \\\\ 0 & 18\\end{bmatrix}$$\n",
    "\n",
    "$$AA^T = \\begin{bmatrix}32 & 0 \\\\ 0 & 18\\end{bmatrix}$$\n",
    "$$U = \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}$$\n",
    "\n",
    "$$A = U\\Sigma V^T = \\begin{bmatrix}1 & 0 \\\\ 0 & 1\\end{bmatrix}\\begin{bmatrix}4\\sqrt{2} & 0 \\\\ 0 & 3\\sqrt{2}\\end{bmatrix}\\frac{1}{\\sqrt{2}}\\begin{bmatrix}1 & 1 \\\\ -1 & 1\\end{bmatrix}$$\n",
    "\n",
    "Example 2:\n",
    "$$A = \\begin{bmatrix}4 & 3 \\\\ 8 & 6\\end{bmatrix}$$\n",
    "\n",
    "$$A^TA = \\begin{bmatrix}80 & 60 \\\\ 60 & 45\\end{bmatrix}$$\n",
    "We can only get one eigenvector from this matrix, because another eigenvalue is 0, to solvw this, we need to find the null space vector:\n",
    "$$V = \\frac{1}{5}\\begin{bmatrix}4 & 3 \\\\ 3 & -4\\end{bmatrix}\\ \\Lambda =\\begin{bmatrix}125 & 0 \\\\ 0 & 0\\end{bmatrix}$$\n",
    "\n",
    "$$AA^T = \\begin{bmatrix}25 & 50 \\\\ 50 & 100\\end{bmatrix}$$\n",
    "$$U = \\frac{1}{\\sqrt{5}}\\begin{bmatrix}1 & 2\\\\ 2 & -1\\end{bmatrix}$$\n",
    "\n",
    "$$A = U\\Sigma V^T = \\frac{1}{\\sqrt{5}}\\begin{bmatrix}1 & 2 \\\\ 2 & -1\\end{bmatrix}\\begin{bmatrix}5\\sqrt{5} & 0 \\\\ 0 & 0\\end{bmatrix}\\frac{1}{5}\\begin{bmatrix}4 & 3 \\\\ 3 & -4\\end{bmatrix}$$\n",
    "\n",
    "Summary, SVD uses 4 fundamental vector space:\n",
    "\n",
    "$$v_i...v_r\\ orthogonal\\ row\\ space$$\n",
    "$$u_i...u_r\\ orthogonal\\ column\\ space$$\n",
    "$$v_{r+1}...v_n\\ orthogonal\\ null\\ space$$\n",
    "$$u_{r+1}...u_n\\ orthogonal\\ A^T\\ null\\ space$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
